dataset: MNIST
seed: 42
device: cuda

test_size: 0.2  # train-test split
val_size: 0.1
batch_size: 128
num_workers: 2

log_every: 20  # compute metrics and log to comet_ml after ... batches
batch_window: 50  # use the previous ... batches to compute the metrics

checkpoint_every: null  # save the model every ... epochs

comet_project: deep-learning-applications
experiment_name: null
experiment_key: null

# Teacher
teacher:
  model_name: MLP
  n_blocks: 2
  hidden_size: 512
  skip: true
  ckp: null  # loading the teacher
  device: cuda  # ok when loading checkpoint to be already in cuda?

# Student
student:
  model_name: MLP
  n_blocks: 1
  hidden_size: 256
  skip: true
  resume_checkpoint: null
  device: cuda

model_name: Distill
num_epochs: 20
learning_rate: 0.01
momentum: 0.9
weight_decay: 0.0005
lr_decay: 0.95

temp: 5.0  # scaling student and teacher logits
w1: 100.0  # weight for soft targets loss
w2: 0.5  # weight for hard targets loss