# Template for the configuration file
# first options are common, mostly, between experiments
# so one can try to merge common options with the specific ones

# seed ensures proper reproducibility, especially when resuming
dataset: MNIST
seed: 42
device: cuda

test_size: 0.2  # train-test split
val_size: 0.1
batch_size: 128
num_workers: 2

learning_rate: 0.01
momentum: 0.9
weight_decay: 0.0005
lr_decay: 0.95  # exponential decay at each epoch

log_every: 20  # compute metrics and log to comet_ml after ... batches
batch_window: 50  # use the previous ... batches to compute the metrics

checkpoint_every: null  # save the model every ... epochs

comet_project: deep-learning-applications

model_name: MLP
num_epochs: 20

resume_checkpoint: null

experiment_name: null
experiment_key: null

# when resuming make sure about to update:
# num_epochs
# resume_checkpoint
# experiment_key